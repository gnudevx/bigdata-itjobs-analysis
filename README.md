# ğŸš€ Big Data IT Jobs Analysis
**Project Structure**
<pre> ```bash
bigdata-itjobs-analysis/
â”‚
â”œâ”€â”€ config-hadoop-hive-spark/   # All for docker-compose + config cho Hadoop, Spark, Hive
â”‚   â”œâ”€â”€ compose.yaml            # Docker Compose file Ä‘á»ƒ dá»±ng cluster Big Data
â”‚   â”œâ”€â”€ hadoop-base/            # Dockerfile & config Hadoop/Spark
â”‚   â”œâ”€â”€ hive/                   # Dockerfile & config Hive Metastore
â”‚   â”œâ”€â”€ master/                 # Config master node (NameNode, ResourceManager)
â”‚   â”œâ”€â”€ slave/                  # Config slave node (DataNode, NodeManager)
â”‚   â””â”€â”€ .devcontainer/          # VSCode devcontainer config
â”‚
â”œâ”€â”€ crawler/                    # crawl collection data + clean data for IT jobs (TopCV, VietnamWorks, ...)
â”‚   â”œâ”€â”€ source_code/            # Script crawl & insert to DB
â”‚   â”‚   â”œâ”€â”€ crawl_topcv.py
â”‚   â”‚   â”œâ”€â”€ crawl_vnwork.py
â”‚   â”‚   â”œâ”€â”€ convert_csv.py
â”‚   â”‚   â”œâ”€â”€ SaoLuu_PhucHoi.py
â”‚   â”‚   â””â”€â”€ Analysis_Data/      # SSAS project (DW, cube, dimensions)
â”‚   â”‚
â”‚   â”œâ”€â”€ Handel_Data_Windows/    # SQL scripts, PowerBI dashboard, data cleaning scripts
â”‚   â”‚   â”œâ”€â”€ Script_Create_Table.sql
â”‚   â”‚   â”œâ”€â”€ TrucQuanHoa.pbix
â”‚   â”‚   â””â”€â”€ CleanData/          # clean data (train model NER)
â”‚   â”‚
â”‚   â””â”€â”€ my_streamlit_app/       # Web visualization báº±ng Streamlit
â”‚       â”œâ”€â”€ Home.py
â”‚       â”œâ”€â”€ page_mapreduce.py
â”‚       â”œâ”€â”€ page_mysql.py
â”‚       â””â”€â”€ mapreduce_jobs/     # Job MapReduce demo (Top10CV, LÆ°Æ¡ng TB, Ká»¹ nÄƒng...)
â”‚
â”œâ”€â”€ hive_scripts/               # folder save HiveQL scripts
â”‚
â”œâ”€â”€ notebooks/                  # Jupyter notebooks (EDA, Spark SQL test, ML pipeline demo)
â”‚
â””â”€â”€ spark_jobs/                 # Spark jobs (ETL, data processing)
    â”œâ”€â”€ Code/
    â”œâ”€â”€ Log/
    â””â”€â”€ Output/ 
``` </pre>
**How to Run the Project**
1ï¸âƒ£ Build & Start Containers
# Go to config folder
cd config-hadoop-hive-spark/

# Build images
docker-compose build --no-cache

# Start cluster
docker compose up -d

# Check running containers
docker ps

2ï¸âƒ£ Hadoop (HDFS) â€“ Basic Commands
- you can entry for docker master and run basic commands
# Put file into HDFS
hdfs dfs -put /tmp/local.csv /user/hadoop/

# List files
hdfs dfs -ls /user/hadoop/

# Read file
hdfs dfs -cat /user/hadoop/local.csv

3ï¸âƒ£ Spark â€“ Run Jobs
# Run Python job
docker exec -it spark-master spark-submit \
  --master yarn /spark_jobs/job.py

# Run Scala/Java JAR
docker exec -it spark-master spark-submit \
  --master yarn /spark_jobs/app.jar

4ï¸âƒ£ Hive â€“ Query Data
docker exec -it hive-server hive

CREATE DATABASE demo;
USE demo;
CREATE TABLE users(id INT, name STRING);
LOAD DATA INPATH '/user/hadoop/users.csv' INTO TABLE users;
SELECT * FROM users;
