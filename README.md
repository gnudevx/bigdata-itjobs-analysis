# ðŸš€ Big Data IT Jobs Analysis
**Project Structure**
<pre> ```bash
bigdata-itjobs-analysis/
â”‚
â”œâ”€â”€ config-hadoop-hive-spark/   # All for docker-compose + config cho Hadoop, Spark, Hive
â”‚   â”œâ”€â”€ compose.yaml            # Docker Compose file Ä‘á»ƒ dá»±ng cluster Big Data
â”‚   â”œâ”€â”€ hadoop-base/            # Dockerfile & config Hadoop/Spark
â”‚   â”œâ”€â”€ hive/                   # Dockerfile & config Hive Metastore
â”‚   â”œâ”€â”€ master/                 # Config master node (NameNode, ResourceManager)
â”‚   â”œâ”€â”€ slave/                  # Config slave node (DataNode, NodeManager)
â”‚   â””â”€â”€ .devcontainer/          # VSCode devcontainer config
â”‚
â”œâ”€â”€ crawler/                    # crawl collection data + clean data for IT jobs (TopCV, VietnamWorks, ...)
â”‚   â”œâ”€â”€ source_code/            # Script crawl & insert to DB
â”‚   â”‚   â”œâ”€â”€ crawl_topcv.py
â”‚   â”‚   â”œâ”€â”€ crawl_vnwork.py
â”‚   â”‚   â”œâ”€â”€ convert_csv.py
â”‚   â”‚   â”œâ”€â”€ SaoLuu_PhucHoi.py
â”‚   â”‚   â””â”€â”€ Analysis_Data/      # SSAS project (DW, cube, dimensions)
â”‚   â”‚
â”‚   â”œâ”€â”€ Handel_Data_Windows/    # SQL scripts, PowerBI dashboard, data cleaning scripts
â”‚   â”‚   â”œâ”€â”€ Script_Create_Table.sql
â”‚   â”‚   â”œâ”€â”€ TrucQuanHoa.pbix
â”‚   â”‚   â””â”€â”€ CleanData/          # clean data (train model NER)
â”‚   â”‚
â”‚   â””â”€â”€ my_streamlit_app/       # Web visualization báº±ng Streamlit
â”‚       â”œâ”€â”€ Home.py
â”‚       â”œâ”€â”€ page_mapreduce.py
â”‚       â”œâ”€â”€ page_mysql.py
â”‚       â””â”€â”€ mapreduce_jobs/     # Job MapReduce demo (Top10CV, LÆ°Æ¡ng TB, Ká»¹ nÄƒng...)
â”‚
â”œâ”€â”€ hive_scripts/               # folder save HiveQL scripts
â”‚
â”œâ”€â”€ notebooks/                  # Jupyter notebooks (EDA, Spark SQL test, ML pipeline demo)
â”‚
â””â”€â”€ spark_jobs/                 # Spark jobs (ETL, data processing)
    â”œâ”€â”€ Code/
    â”œâ”€â”€ Log/
    â””â”€â”€ Output/ 
``` </pre>
## ðŸš€ How to Run the Project

| Step | Description              | Command                                                                 |
|:----:|--------------------------|-------------------------------------------------------------------------|
| **1** | Go to config folder      | `cd config-hadoop-hive-spark/`                                         |
|       |                          | `docker build -t hadoop-base1 ./hadoop-base`                           |
|      | Build images             | `docker compose build --no-cache`                                       |
|      | Start cluster            | `docker compose up -d`                                                  |
|      | Check running containers | `docker ps`                                                             |
| **2** | Enter Hadoop master     | `docker exec -it hadoop-master bash`                                    |
|      | Put file into HDFS       | `hdfs dfs -put /tmp/local.csv /user/hadoop/`                            |
|      | List files               | `hdfs dfs -ls /user/hadoop/`                                            |
|      | Read file                | `hdfs dfs -cat /user/hadoop/local.csv`                                  |
| **3** | Run Python job          | `docker exec -it spark-master spark-submit --master yarn /spark_jobs/job.py` |
|      | Run Scala/Java JAR       | `docker exec -it spark-master spark-submit --master yarn /spark_jobs/app.jar` |
| **4** | Open Hive CLI           | `docker exec -it hive-server hive`                                      |
|      | entry first              | `schematool -dbType mysql -initSchema`                                  |
|      | start service             | `hive --service metastore &`                                           |
|      | Create database          | `CREATE DATABASE demo;`                                                 |
|      | Use database             | `USE demo;`                                                             |
|      | Create table             | `CREATE TABLE users(id INT, name STRING);`                              |
|      | Load data                | `LOAD DATA INPATH '/user/hadoop/users.csv' INTO TABLE users;`           |
|      | Query data               | `SELECT * FROM users;`                
