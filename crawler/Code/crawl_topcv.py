# topcv_crawler.py
import json
import time
import random
import re
import os
import logging
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium_stealth import stealth
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.chrome.service import Service
from datetime import datetime
from bs4 import BeautifulSoup
# ---------------------------
# Config
# ---------------------------
BASE_IT = "https://www.topcv.vn/viec-lam-it"
TARGET_PER_GROUP = 500
DATA_DIR = "/opt/airflow/crawler/Dataset"
LOG_DIR = "/opt/airflow/crawler/Logs"

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

TODAY = datetime.now().strftime("%Y-%m-%d")
OUTPUT_FILE = os.path.join(DATA_DIR, f"topcv_{TODAY}.json")
LOG_FILE = os.path.join(LOG_DIR, f"topcv_{TODAY}.log")

logging.basicConfig(
    filename=LOG_FILE,
    filemode="a",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

def log(msg):
    print(msg)
    logging.info(msg)

def human_delay(base=1.0, variation=0.5):
    time.sleep(base + random.random() * variation)

# ---------------------------
# Init driver Linux + Docker
# ---------------------------
def init_driver(profile_dir=None, proxy=None, headless=True, version_main=None):
    import random

    opts = uc.ChromeOptions()
    if profile_dir:
        opts.add_argument(f"--user-data-dir={profile_dir}")
    if proxy:
        opts.add_argument(f"--proxy-server={proxy}")

    # ‚öôÔ∏è B·∫Øt bu·ªôc c√≥
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--disable-extensions")
    opts.add_argument("--disable-popup-blocking")
    opts.add_argument("--window-size=1920,1080")

    # ‚ö†Ô∏è B·ªè d·∫•u hi·ªáu headless Chrome m·∫∑c ƒë·ªãnh
    if headless:
        opts.add_argument("--headless=new")
        opts.add_argument("--window-size=1920,1080")
        opts.add_argument("--disable-blink-features=AutomationControlled")

    # üë§ Random user-agent m·ªói l·∫ßn
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_0) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
    ]
    opts.add_argument(f"--user-agent={random.choice(user_agents)}")

    service = Service("/usr/local/bin/chromedriver")

    driver = uc.Chrome(
        options=opts,
        service=service,
        use_subprocess=True,
        headless=headless,
        version_main=version_main,
    )

    # üîí Fake fingerprint (gi√∫p qua Cloudflare)
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": """
        Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
        Object.defineProperty(navigator, 'plugins', { get: () => [1,2,3,4,5] });
        Object.defineProperty(navigator, 'languages', { get: () => ['en-US','en'] });
        Object.defineProperty(navigator, 'platform', { get: () => 'Linux x86_64' });
        Object.defineProperty(navigator, 'vendor', { get: () => 'Google Inc.' });
        Object.defineProperty(navigator, 'hardwareConcurrency', { get: () => 8 });
        Object.defineProperty(navigator, 'deviceMemory', { get: () => 8 });
        """
    })
    stealth(
        driver,
        languages=["en-US", "en"],
        vendor="Google Inc.",
        platform="Linux x86_64",
        webgl_vendor="Intel Inc.",
        renderer="Intel Iris OpenGL Engine",
        fix_hairline=True,
    )
    # üí§ Delay kh·ªüi ƒë·ªông ƒë·ªÉ Cloudflare kh√¥ng nghi
    time.sleep(random.uniform(3, 6))
    return driver


# ---------------------------
# Scroll page
# ---------------------------
def human_scroll(driver):
    try:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.3);")
        human_delay(0.5, 0.3)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.7);")
        human_delay(0.5, 0.3)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        human_delay(0.5, 0.3)
    except Exception as e:
        log(f"‚ö†Ô∏è Scroll l·ªói: {e}")

# ---------------------------
# Crawl danh s√°ch k·ªπ nƒÉng
# ---------------------------
def get_skills_info(driver, retries=3):
    for attempt in range(retries):
        driver.get(BASE_IT)
        log(f"üîó M·ªü trang {BASE_IT}")
        time.sleep(6)

        html = driver.page_source.lower()
        if "just a moment" in html or "challenge" in driver.current_url:
            log(f"‚ö†Ô∏è Cloudflare ch·∫∑n l·∫ßn {attempt+1}/{retries} ‚Äî th·ª≠ l·∫°i...")
            time.sleep(8)
            continue

        try:
            WebDriverWait(driver, 40).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.list-top-skill"))
            )
            break
        except TimeoutException:
            log("‚è≥ H·∫øt th·ªùi gian ch·ªù load k·ªπ nƒÉng.")
            if attempt < retries - 1:
                continue
            raise

    btns = driver.find_elements(By.CSS_SELECTOR, "div.list-top-skill button")
    skills = []
    for btn in btns:
        raw = btn.text.strip()
        name = re.sub(r"\s*\d+$", "", raw).strip()
        sid = btn.get_attribute("data-skill-id") or btn.get_attribute("data-skill-id-other")
        if sid:
            skills.append((name, sid))
            log(f"‚Üí Nh√≥m '{name}' (skill_id={sid})")
    return skills

# ---------------------------
# Next page
# ---------------------------
def next_page(driver):
    try:
        soup = BeautifulSoup(driver.page_source, "html.parser")
        pagination = soup.select_one("ul.pagination")
        if not pagination:
            log("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ph·∫ßn ph√¢n trang (pagination).")
            return False

        # T√¨m trang hi·ªán t·∫°i
        active = pagination.select_one("li.active span")
        if not active:
            log("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y trang hi·ªán t·∫°i trong ph√¢n trang.")
            return False

        current_page = active.text.strip()
        next_li = active.find_parent("li").find_next_sibling("li")

        # N·∫øu kh√¥ng c√≤n li k·∫ø ti·∫øp ho·∫∑c li k·∫ø ti·∫øp kh√¥ng c√≥ link
        if not next_li or not next_li.find("a"):
            log("‚õî Kh√¥ng c√≤n n√∫t Next (ƒë√£ ƒë·∫øn trang cu·ªëi).")
            return False

        next_url = next_li.find("a")["href"]
        if not next_url.startswith("http"):
            next_url = "https://www.topcv.vn" + next_url

        log(f"‚û°Ô∏è Chuy·ªÉn sang trang k·∫ø ti·∫øp: {next_url}")
        driver.get(next_url)
        human_delay(2.0, 0.5)
        return True

    except Exception as e:
        log(f"‚ö†Ô∏è L·ªói khi chuy·ªÉn trang: {e}")
        return False

# ---------------------------
# Crawl jobs
# ---------------------------
def scrape_jobs_on_current_filter(driver, sid, target_count=50):
    jobs, seen = [], set()
    max_pages = 10  # Gi·ªõi h·∫°n t·ªëi ƒëa (40 trang ƒë·ªÉ d∆∞)
    all_links = []

    empty_pages = 0  # üß© ƒê·∫øm s·ªë trang li√™n ti·∫øp kh√¥ng c√≥ job

    # Duy·ªát trang 1 (r·ªóng) + c√°c trang ti·∫øp theo
    for page in [""] + list(range(2, max_pages + 1)):
        if page == "":
            url = f"https://www.topcv.vn/viec-lam-it?sort=&skill_id={sid}&skill_id_other=&keyword=&position=&salary="
        else:
            url = f"https://www.topcv.vn/viec-lam-it?sort=&skill_id={sid}&skill_id_other=&keyword=&position=&salary=&page={page}"

        log(f"-- Trang {page or 'r·ªóng'}: {url}")
        driver.get(url)
        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.job-item-2 h3.title a[target='_blank']"))
            )
        except:
            log("‚ö†Ô∏è Trang kh√¥ng c√≥ job (ch·ªù timeout).")

        time.sleep(1)
        cards = driver.find_elements(By.CSS_SELECTOR, "div.job-item-2 h3.title a[target='_blank']")
        page_links = [a.get_attribute("href") for a in cards if a.get_attribute("href")]

        # üß© Ki·ªÉm tra n·∫øu kh√¥ng c√≥ job n√†o
        if not page_links:
            empty_pages += 1
            log(f"‚ö†Ô∏è Trang kh√¥ng c√≥ job ({empty_pages}/2) ‚Äî th·ª≠ trang k·∫ø ti·∫øp...")
            if empty_pages >= 2:
                log("üö´ D·ª´ng: 2 trang li√™n ti·∫øp kh√¥ng c√≥ job.")
                break
            continue
        else:
            empty_pages = 0  # reset l·∫°i v√¨ trang n√†y c√≥ job

        new_links = [l for l in page_links if l not in seen]
        if not new_links:
            log("‚ö†Ô∏è Kh√¥ng c√≥ link m·ªõi ‚Äî d·ª´ng l·∫°i.")
            break

        log(f"üîó T√¨m th·∫•y {len(new_links)} link job m·ªõi tr√™n trang {page or 1}")
        all_links.extend(new_links)
        seen.update(new_links)

        # D·ª´ng n·∫øu ƒë√£ ƒë·ªß s·ªë l∆∞·ª£ng job m·ª•c ti√™u
        if len(all_links) >= target_count:
            break
    # üß© In t·ªïng k·∫øt
    log(f"üß© T·ªïng {len(all_links)} link job thu ƒë∆∞·ª£c cho skill {sid}")
    # 2Ô∏è‚É£ Crawl chi ti·∫øt t·ª´ng job trong tab m·ªõi
    for i, link in enumerate(all_links[:target_count]):
        try:
            driver.execute_script(f"window.open('{link}', '_blank');")
            driver.switch_to.window(driver.window_handles[-1])
            log(f"üëâ Crawl job ({i+1}/{len(all_links)}): {link}")

            WebDriverWait(driver, 25).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "h1"))
            )
            human_scroll(driver)
            retry_count = 0
            while "just a moment" in driver.title.lower() and retry_count < 3:
                log(f"‚è≥ Cloudflare ch·∫∑n, ch·ªù 2s v√† reload l·∫ßn {retry_count+1}")
                time.sleep(2)
                driver.refresh()
                retry_count += 1

            # N·∫øu sau 3 l·∫ßn v·∫´n b·ªã ch·∫∑n, b·ªè qua job
            if "just a moment" in driver.title.lower():
                log("‚ö†Ô∏è Trang b·ªã ch·∫∑n b·ªüi Cloudflare ‚Äî b·ªè qua job n√†y.")
                driver.close()
                driver.switch_to.window(driver.window_handles[0])
                human_delay(1.5, 0.7)
                continue

            # ‚Äî ph·∫ßn crawl job chi ti·∫øt ch·∫°y ·ªü ƒë√¢y ‚Äî
            try:
                title_elem = driver.find_element(By.CSS_SELECTOR, "h1.job-detail__info--title a")
                title = title_elem.text.strip()
                title = re.sub(r"\s*\(.*?\)", "", title).strip()
            except Exception:
                title = driver.title.strip()
                if "topcv.vn" in title.lower():
                    title = "(no title)"
            info = {"salary": "", "location": "", "experience": ""}
            for sec in driver.find_elements(By.CSS_SELECTOR, ".job-detail__info--section"):
                try:
                    key = sec.find_element(By.CSS_SELECTOR, ".job-detail__info--section-content-title").text.lower()
                    val = sec.find_element(By.CSS_SELECTOR, ".job-detail__info--section-content-value").text.strip()
                    if "l∆∞∆°ng" in key:
                        info["salary"] = val
                    elif "ƒë·ªãa ƒëi·ªÉm" in key:
                        info["location"] = val
                    elif "kinh nghi·ªám" in key:
                        info["experience"] = val
                except:
                    continue

            desc = {"description": "", "requirements": "", "benefits": "", "work_location_detail": "", "working_time": ""}
            for item in driver.find_elements(By.CSS_SELECTOR, ".job-description__item"):
                try:
                    h = item.find_element(By.TAG_NAME, "h3").text.lower()
                    content = item.find_element(By.CSS_SELECTOR, ".job-description__item--content").text.strip()
                    if "th·ªùi gian l√†m vi·ªác" in h:
                        desc["working_time"] = content
                    elif "m√¥ t·∫£ c√¥ng vi·ªác" in h:
                        desc["description"] = content
                    elif "y√™u c·∫ßu ·ª©ng vi√™n" in h:
                        desc["requirements"] = content
                    elif "quy·ªÅn l·ª£i" in h:
                        desc["benefits"] = content
                    elif "ƒë·ªãa ƒëi·ªÉm l√†m vi·ªác" in h:
                        desc["work_location_detail"] = content
                except:
                    continue

            try:
                deadline = driver.find_element(By.CSS_SELECTOR, "div.job-detail__information-detail--actions-label").text.strip()
            except:
                deadline = ""

            jobs.append({"title": title, "link": link, **info, **desc, "deadline": deadline})
            log(f"‚úÖ ƒê√£ l·∫•y job: {title}")

        except Exception as e:
            log(f"‚ùå L·ªói khi crawl job {link}: {e}")

        finally:
            driver.close()
            driver.switch_to.window(driver.window_handles[0])
            human_delay(1.5, 0.7)

    log(f"üéØ ƒê√£ crawl {len(jobs)} jobs cho filter {sid}")
    return jobs[:target_count]

# ---------------------------
# Ghi JSON
# ---------------------------
def append_to_json_file(data, filepath):
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    existing = []
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            try:
                existing = json.load(f)
            except:
                pass
    existing.append(data)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(existing, f, ensure_ascii=False, indent=2)

# ---------------------------
# Main
# ---------------------------
def run_topcv_crawler():
    driver = init_driver(headless=True)
    skills = get_skills_info(driver)
    for name, sid in skills:
        log(f"\n=== Crawl nh√≥m {name} ===")
        driver.get(BASE_IT)
        human_delay()
        try:
            jobs = scrape_jobs_on_current_filter(driver, sid, TARGET_PER_GROUP)
            append_to_json_file({"group": name, "jobs": jobs}, OUTPUT_FILE)
            log(f"‚úÖ ƒê√£ l∆∞u nh√≥m {name} v√†o {OUTPUT_FILE}")
        except Exception as e:
            log(f"‚ùå L·ªói nh√≥m {name}: {e}")
            continue
    driver.quit()

if __name__ == "__main__":
    run_topcv_crawler()
