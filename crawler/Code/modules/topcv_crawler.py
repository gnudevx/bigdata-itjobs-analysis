# topcv_crawler.py
import re
import os
import json
import time
from datetime import datetime
# from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import InvalidSessionIdException
import shutil
# Import d√πng chung
from Code.core.driver_for_topcv import init_topcv_driver
from Code.core.utils import setup_logger, log_and_print, human_delay, save_json
from Code.config.settings import get_output_file, BASE_IT_TOPCV, TARGET_PER_GROUP, LOG_DIR

logger = setup_logger("topcv")

# ---------------------------
# Scroll page
# ---------------------------
def human_scroll(driver):
    try:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.3);")
        human_delay(0.5, 0.3)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.7);")
        human_delay(0.5, 0.3)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        human_delay(0.5, 0.3)
    except Exception as e:
        log_and_print(f"‚ö†Ô∏è Scroll l·ªói: {e}", logger)

# ---------------------------
# Crawl danh s√°ch k·ªπ nƒÉng
# ---------------------------
def get_skills_info(driver):
    retries = 0
    while retries < 3:
        try:
            log_and_print(f"üåê ƒêang m·ªü trang k·ªπ nƒÉng (l·∫ßn {retries+1})...", logger)
            driver.get(BASE_IT_TOPCV)
            WebDriverWait(driver, 40).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.list-top-skill"))
            )
            break
        except Exception as e:
            log_and_print(f"‚ö†Ô∏è L·ªói khi load trang k·ªπ nƒÉng: {e}", logger)
            retries += 1
            time.sleep(3)
    else:
        raise Exception("Kh√¥ng th·ªÉ t·∫£i trang k·ªπ nƒÉng sau 3 l·∫ßn th·ª≠")

    btns = driver.find_elements(By.CSS_SELECTOR, "div.list-top-skill button")
    skills = []
    for btn in btns:
        raw = btn.text.strip()
        name = re.sub(r"\s*\d+$", "", raw).strip()
        sid = btn.get_attribute("data-skill-id") or btn.get_attribute("data-skill-id-other")
        if sid:
            skills.append((name, sid))
            log_and_print(f"‚Üí Nh√≥m '{name}' (skill_id={sid})", logger)
    return skills

# ---------------------------
# Crawl jobs 
# ---------------------------
def scrape_jobs_on_current_filter_single_tab(driver, sid, target_count=50):
    jobs, seen = [], set()
    all_links = []
    empty_pages = 0

    for page in [""] + list(range(2, 11)):
        if sid == "other":
            url = f"{BASE_IT_TOPCV}?skill_id=&skill_id_other=other"
            if page != "":
                url += f"&page={page}"
        else:
            url = f"{BASE_IT_TOPCV}?skill_id={sid}"
            if page != "":
                url += f"&page={page}"
        log_and_print(f"-- Trang {page or 1}: {url}", logger)

        try:
            driver.get(url)
            WebDriverWait(driver, 20).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.job-item-2 h3.title a[target='_blank']"))
            )
        except TimeoutException:
            log_and_print(f"‚ö†Ô∏è Timeout khi t·∫£i trang {page or 1}", logger)

        cards = driver.find_elements(By.CSS_SELECTOR, "div.job-item-2 h3.title a[target='_blank']")
        page_links = [a.get_attribute("href") for a in cards if a.get_attribute("href")]

        if not page_links:
            empty_pages += 1
            log_and_print(f"‚ö†Ô∏è Trang {page or 1} kh√¥ng c√≥ job ({empty_pages}/2) ‚Äî th·ª≠ trang k·∫ø ti·∫øp...", logger)
            if empty_pages >= 2:
                log_and_print("üö´ D·ª´ng: G·∫∑p 2 trang li√™n ti·∫øp kh√¥ng c√≥ job ‚Üí k·∫øt th√∫c v√≤ng l·∫∑p.", logger)
                break
            continue
        else:
            empty_pages = 0

        new_links = [l for l in page_links if l not in seen]
        if not new_links:
            log_and_print("‚ö†Ô∏è Kh√¥ng c√≥ job m·ªõi (to√†n link tr√πng l·∫∑p). D·ª´ng crawl trang k·∫ø ti·∫øp.", logger)
            break
        all_links.extend(new_links)
        seen.update(new_links)
        if len(all_links) >= target_count:
            log_and_print(f"üéØ ƒê·ªß {target_count} job ‚Üí d·ª´ng t√¨m th√™m trang.", logger)
            break

    # Crawl t·ª´ng job tr√™n c√πng 1 tab
    for i, link in enumerate(all_links[:target_count]):
        try:
            driver.get(link)
            log_and_print(f"üëâ Crawl job ({i+1}/{len(all_links)}): {link}")

            try:
                WebDriverWait(driver, 25).until(
                    lambda d: "just a moment" not in d.title.lower()
                )
            except:
                log_and_print("‚ö†Ô∏è Trang load qu√° l√¢u ho·∫∑c b·ªã ch·∫∑n Cloudflare", logger)

            human_scroll(driver)

            # N·∫øu v·∫´n c√≤n b·ªã Cloudflare ‚Äî th·ª≠ reload 3 l·∫ßn
            retry_count = 0
            while "just a moment" in driver.title.lower() and retry_count < 3:
                log_and_print(f"‚è≥ Cloudflare ch·∫∑n, ch·ªù 2s v√† reload l·∫ßn {retry_count+1}", logger)
                time.sleep(2)
                driver.refresh()
                retry_count += 1

            if "just a moment" in driver.title.lower():
                log_and_print("‚ö†Ô∏è Trang b·ªã ch·∫∑n b·ªüi Cloudflare ‚Äî b·ªè qua job n√†y.", logger)
                continue

            # Crawl job chi ti·∫øt
            try:
                title_elem = driver.find_element(By.CSS_SELECTOR, "h1.job-detail__info--title a")
                title = title_elem.text.strip()
                title = re.sub(r"\s*\(.*?\)", "", title).strip()
            except Exception:
                title = driver.title.strip()
                if "topcv.vn" in title.lower():
                    title = "(no title)"

            info = {"salary": "", "location": "", "experience": ""}
            for sec in driver.find_elements(By.CSS_SELECTOR, ".job-detail__info--section"):
                try:
                    key = sec.find_element(By.CSS_SELECTOR, ".job-detail__info--section-content-title").text.lower()
                    val = sec.find_element(By.CSS_SELECTOR, ".job-detail__info--section-content-value").text.strip()
                    if "l∆∞∆°ng" in key:
                        info["salary"] = val
                    elif "ƒë·ªãa ƒëi·ªÉm" in key:
                        info["location"] = val
                    elif "kinh nghi·ªám" in key:
                        info["experience"] = val
                except:
                    continue

            desc = {"description": "", "requirements": "", "benefits": "", "work_location_detail": "", "working_time": ""}
            for item in driver.find_elements(By.CSS_SELECTOR, ".job-description__item"):
                try:
                    h = item.find_element(By.TAG_NAME, "h3").text.lower()
                    content = item.find_element(By.CSS_SELECTOR, ".job-description__item--content").text.strip()
                    if "th·ªùi gian l√†m vi·ªác" in h:
                        desc["working_time"] = content
                    elif "m√¥ t·∫£ c√¥ng vi·ªác" in h:
                        desc["description"] = content
                    elif "y√™u c·∫ßu ·ª©ng vi√™n" in h:
                        desc["requirements"] = content
                    elif "quy·ªÅn l·ª£i" in h:
                        desc["benefits"] = content
                    elif "ƒë·ªãa ƒëi·ªÉm l√†m vi·ªác" in h:
                        desc["work_location_detail"] = content
                except:
                    continue

            try:
                deadline = driver.find_element(By.CSS_SELECTOR, "div.job-detail__information-detail--actions-label").text.strip()
            except:
                deadline = ""

            jobs.append({"title": title, "link": link, **info, **desc, "deadline": deadline})
            log_and_print(f"‚úÖ ƒê√£ l·∫•y job: {title}", logger)

        except Exception as e:
            log_and_print(f"‚ùå L·ªói khi crawl job {link}: {e}", logger)

        human_delay(1.5, 0.7)

    log_and_print(f"üéØ ƒê√£ crawl {len(jobs)} jobs cho filter {sid}", logger)
    return jobs[:target_count]
# ---------------------------
# Main function for airflow DAG
# ---------------------------
def run_topcv_crawler():
    try:
        driver = init_topcv_driver(headless=True)
        skills = get_skills_info(driver)
        output_file = get_output_file("topcv")
        log_file = os.path.join(LOG_DIR, f"topcv_{datetime.now().strftime('%Y-%m-%d')}.log")
        logger = setup_logger("topcv_logger", log_file)
        for name, sid in skills:
            log_and_print(f"\n=== Crawl nh√≥m {name} ===", logger)
            jobs = scrape_jobs_on_current_filter_single_tab(driver, sid, TARGET_PER_GROUP)
            save_json({"group": name, "jobs": jobs}, output_file)
            log_and_print(f"‚úÖ L∆∞u nh√≥m {name}", logger)
    finally:
        driver.quit()
        # cleanup temp UC cache
        shutil.rmtree(os.environ.get("UDC_DATA_DIR", ""), ignore_errors=True)
if __name__ == "__main__":
    run_topcv_crawler()
