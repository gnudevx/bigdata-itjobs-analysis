import os
import time
import json
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
from selenium_stealth import stealth

# Import n·ªôi b·ªô
from Code.core.utils import save_json
from Code.core.driver_for_vnwork import init_vnwork_driver
from Code.config.settings import get_output_file, BASE_IT_VNWORK, LOG_DAY_DIR


# ==========================================================
# üß© H√ÄM PH·ª§ TR·ª¢
# ==========================================================

def parse_deadline(deadline_text: str, crawl_date=None):
    """
    Chuy·ªÉn ƒë·ªïi chu·ªói th·ªùi gian nh∆∞ 'H·∫øt h·∫°n trong 3 ng√†y' th√†nh ng√†y c·ª• th·ªÉ.
    """
    if crawl_date is None:
        crawl_date = datetime.today()

    text = deadline_text.lower().strip()

    if "h·∫øt h·∫°n trong" in text:
        # V√≠ d·ª•: "H·∫øt h·∫°n trong 1 th√°ng", "H·∫øt h·∫°n trong 5 ng√†y"
        parts = text.replace("h·∫øt h·∫°n trong", "").strip().split()
        if len(parts) >= 2:
            num = int(parts[0])
            unit = parts[1]

            if "ng√†y" in unit:
                expire_date = crawl_date + timedelta(days=num)
            elif "tu·∫ßn" in unit:
                expire_date = crawl_date + timedelta(weeks=num)
            elif "th√°ng" in unit:
                expire_date = crawl_date + relativedelta(months=num)
            else:
                return deadline_text  # Kh√¥ng parse ƒë∆∞·ª£c

            return f"H·∫°n n·ªôp h·ªì s∆°: {expire_date.strftime('%d/%m/%Y')}"

    # N·∫øu text ƒë√£ l√† ng√†y c·ª• th·ªÉ s·∫µn r·ªìi
    return deadline_text


def get_info(url, driver):
    """
    L·∫•y th√¥ng tin chi ti·∫øt cho 1 job c·ª• th·ªÉ.
    """
    driver.get(url)
    time.sleep(1)

    # Scroll xu·ªëng cu·ªëi trang ƒë·ªÉ load ƒë·∫ßy ƒë·ªß n·ªôi dung
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(0.5)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height

    soup = BeautifulSoup(driver.page_source, "html.parser")

    # === Th√¥ng tin c∆° b·∫£n ===
    job_title = soup.find("h1", class_="sc-ab270149-0 hAejeW")
    deadline = soup.find("span", class_="sc-ab270149-0 ePOHWr")
    salary = soup.find("span", class_="sc-ab270149-0 cVbwLK")
    location = soup.find("div", class_="sc-a137b890-1 joxJgK")

    job_title = job_title.text.strip() if job_title else "N/A"
    print("Job title: ", job_title)
    deadline = deadline.text.strip() if deadline else "N/A"
    salary = salary.text.strip() if salary else "N/A"
    location = location.text.strip() if location else "N/A"

    # Chu·∫©n h√≥a deadline -> ng√†y c·ª• th·ªÉ
    deadline = parse_deadline(deadline, crawl_date=datetime.today())

    # === M√¥ t·∫£ & Y√™u c·∫ßu ===
    job_description, job_requirement = "", ""
    detail_blocks = soup.find_all("div", class_="sc-1671001a-3 hmvhgA")

    for block in detail_blocks:
        headings = block.find_all("h2", class_="sc-1671001a-5 cjuZti")
        for heading in headings:
            heading_text = heading.get_text(strip=True)
            next_div = heading.find_next_sibling("div")

            if next_div:
                clean_text = next_div.get_text(separator="\n").strip()
                if "M√¥ t·∫£ c√¥ng vi·ªác" in heading_text:
                    job_description = clean_text
                elif "Y√™u c·∫ßu c√¥ng vi·ªác" in heading_text:
                    job_requirement = clean_text

    # === Ph√∫c l·ª£i ===
    benefits = soup.find_all("div", class_="sc-c683181c-2 fGxLZh")
    benefit = "".join([f"- {b.text.strip()}\n" for b in benefits])

    # === Th√¥ng tin th√™m ===
    experience_value, work_day_value = "", ""
    info_blocks = soup.find_all("div", class_="sc-7bf5461f-0 dHvFzj")

    for block in info_blocks:
        labels = block.find_all("label")
        for label in labels:
            label_text = label.get_text(strip=True)
            next_p = label.find_next_sibling("p")

            if label_text == "S·ªê NƒÇM KINH NGHI·ªÜM T·ªêI THI·ªÇU":
                experience_value = next_p.get_text(strip=True) if next_p else ""
            elif label_text == "NG√ÄY L√ÄM VI·ªÜC":
                work_day_value = next_p.get_text(strip=True) if next_p else ""

    # Tr·∫£ k·∫øt qu·∫£
    return {
        "title": job_title,
        "link": url,
        "salary": salary,
        "location": location,
        "experience": experience_value,
        "description": job_description,
        "requirements": job_requirement,
        "benefits": benefit,
        "work_location_detail": location,
        "working_time": work_day_value,
        "deadline": deadline,
    }


# ==========================================================
# üß≠ H√ÄM CH√çNH CRAWL
# ==========================================================

def scrape_jobs_by_skill(driver, skill_name, skill_url, max_pages=10):
    """Crawl t·∫•t c·∫£ job thu·ªôc m·ªôt skill, t·ª± skip n·∫øu trang l·ªói li√™n ti·∫øp."""
    jobs = []
    page = 1
    fail_streak = 0

    while page <= max_pages:
        print(f"[INFO] Scraping skill={skill_name}, page={page}")
        try:
            driver.get(f"{skill_url}&page={page}")
            time.sleep(1.5)
            fail_streak = 0  # reset khi th√†nh c√¥ng
        except Exception as e:
            print(f"[WARN] Page {page} failed for {skill_name}: {e}")
            fail_streak += 1
            if fail_streak >= 3:
                print(f"[WARN] ‚ùå Qu√° nhi·ªÅu l·ªói li√™n ti·∫øp, d·ª´ng k·ªπ nƒÉng {skill_name}.")
                break
            page += 1
            continue

        soup = BeautifulSoup(driver.page_source, "html.parser")
        job_containers = soup.find_all("div", class_="sc-iVDsrp frxvCT")
        if not job_containers:
            print(f"[INFO] No more jobs found at page {page}")
            break

        for container in job_containers:
            a_tag = container.find("a", class_="img_job_card")
            if not a_tag:
                continue
            url = a_tag.get("href")
            if not url:
                continue

            full_url = "https://www.vietnamworks.com" + url
            try:
                info = get_info(full_url, driver)
                jobs.append(info)
            except Exception as e:
                print(f"[ERROR] Failed to crawl job at {full_url}: {e}")
                continue

        page += 1
        time.sleep(1.0 + (page % 3) * 0.5)

    return jobs

def run_vnwork_crawler():
    driver = init_vnwork_driver()

    print("[INFO] Opening Vietnamworks...")
    driver.get(BASE_IT_VNWORK)
    time.sleep(10)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    skills_div = soup.find("div", class_="skill-tag-details")
    if not skills_div:
        print("[ERROR] Kh√¥ng t√¨m th·∫•y danh s√°ch k·ªπ nƒÉng. C√≥ th·ªÉ b·ªã ch·∫∑n.")
        driver.quit()
        return

    skill_elements = skills_div.find_all("div", class_="tag-wrapper")
    output_path = get_output_file(prefix="vnwork")
    print(f"[INFO] Found {len(skill_elements)} skills, saving to {output_path}")

    for idx, skill_el in enumerate(skill_elements, 1):
        skill_name = skill_el.get_text(strip=True)
        skill_url = f"https://www.vietnamworks.com/viec-lam?q={skill_name.lower().replace(' ', '-')}"

        print(f"\n=== [{idx}/{len(skill_elements)}] Crawling {skill_name} ===")

        # üëâ Kh·ªüi t·∫°o driver ri√™ng cho t·ª´ng k·ªπ nƒÉng
        driver = init_vnwork_driver()
        time.sleep(3)

        try:
            jobs = scrape_jobs_by_skill(driver, skill_name, skill_url)
            if jobs:
                save_json([{"group": skill_name, "jobs": jobs}], output_path)
                print(f"[SAVE] {len(jobs)} jobs saved for {skill_name}")
            else:
                print(f"[WARN] No jobs found for {skill_name}")
        except Exception as e:
            print(f"[ERROR] Skill {skill_name} failed: {e}")
        finally:
            driver.quit()
            os.system("pkill -f chrome || true")  # üßπ ƒë·∫£m b·∫£o kill Chrome zombie
            time.sleep(2)
    print(f"[DONE] ‚úÖ T·∫•t c·∫£ d·ªØ li·ªáu ƒë√£ l∆∞u v√†o {output_path}")

# ==========================================================
# üèÅ ENTRYPOINT
# ==========================================================
if __name__ == "__main__":
    run_vnwork_crawler()
